THIRD PARTY TOOLS:
------
You  have access to the following tools:

# Libraries
from PIL import Image
import numpy as np
import scipy
import torch
import cv2
import math
from typing import Union

IMPLEMENTED TOOLS:
------
You have access to the following tools:

# First Level: File IO
templates = {} # dictionary to store and cache the multi-modality instruction
# possible keys in templates: "scene", "dragged_obj", "base_obj"
# NOTE: the word in one instruction inside {} stands for the visual part of the instruction and will be obtained with get operation
# Example: {scene} -> templates.get('scene')
BOUNDS = {} # dictionary to store action space boundary

def GetObsImage(obs) -> Image.Image:
    """Get the current image to start the system.
    Examples:
        image = GetObsImage(obs)
    """
    pass

def SaveFailureImage() -> str:
    """Save images when execution fails
    Examples:
        info = SaveFailureImage()
    """
    pass

# Second Level: Core Modules
## Perception Modules
def SAM(image: Image.Image) -> dict:
    """Get segmentation results with SAM
    Examples:
        masks = SAM(image=image)
    """
    pass

def ImageCrop(image: Image.Image, masks: dict):
    """Crop image with given masks
    Examples:
        objs, masks = ImageCrop(image=image, masks=masks)
    """
    pass

def CLIPRetrieval(objs: list, query: str | Image.Image , pre_obj1: int = None, pre_obj2: int = None) -> np.ndarray:
    """Retrieve the desired object(s) with CLIP, the query could be string or an image
    Examples:
        obj_0 = CLIPRetrieval(objs=objs, query='the yellow and purple polka dot pan') # the query is a string
        obj_0 = CLIPRetrieval(objs=objs, query=templates['dragged_obj']) # the query is image, stored in templates
    """
    pass

def get_objs_match(objs_list1: list, objs_list2: list) -> tuple:
    """Get correspondences of objects between two lists using the Hungarian Algorithm"""
    return (list, list)

## Action Modules
def Pixel2Loc(obj: np.ndarray, masks: np.ndarray) -> np.ndarray:
    """Map masks to specific locations"""
    pass

def PickPlace(pick: np.ndarray, place: np.ndarray, bounds: np.ndarray, yaw_angle_degree: float = None, tool: str = "suction") -> str:
    """Pick and place the object based on given locations and bounds"""
    pass

def DistractorActions(mask_obs: list, obj_list: list, tool: str = "suction") -> list:
    """Remove observed objects that conflict with the goal object list"""
    pass

def RearrangeActions(pick_masks: list, place_masks: list, pick_ind: list, place_ind: list, bounds: np.ndarray, tool: str = "suction") -> list:
    """Composite multiple pick and place actions"""
    pass

# Third Level: Connect to Robotic Hardware
def RobotExecution(action) -> dict
    """Execute the robot, then return the exectation result as dict """
    pass

Examples:
------
Use the following examples to understand tools:
## Example 1
# Instruction: Put the checkerboard round into the yellow and purple polka dot pan.
def main_1() -> dict:
    """Execute the given instructions of placing the checkerboard round into the yellow and purple polka dot pan"""
    image = GetObsImage(obs)
    masks = SAM(image=image)
    objs, masks = ImageCrop(image=image, masks=masks)
    obj_0 = CLIPRetrieval(objs=objs, query='the yellow and purple polka dot pan')
    loc_0 = Pixel2Loc(obj=obj_0, masks=masks)
    obj_1 = CLIPRetrieval(objs=objs, query='the checkerboard round', pre_obj1=obj_0)
    loc_1 = Pixel2Loc(obj=obj_1, masks=masks)
    action = PickPlace(pick=loc_1, place=loc_0, bounds=BOUNDS)
    info = RobotExecution(action=action)
    return info

## Example 2:
# Instruction: Rotate the {dragged_obj} 150 degrees.
def main_2() -> dict:
    """Execute the given instructions of rotating the {dragged_obj} 150 degrees"""
    image = GetObsImage(obs)
    masks = SAM(image=image)
    objs, masks = ImageCrop(image=image, masks=masks)
    obj_0 = CLIPRetrieval(objs=objs, query=templates.get("dragged_obj"))
    loc_0 = Pixel2Loc(obj=obj_0, masks=masks)
    action = PickPlace(pick=loc_0, place=loc_0, bounds=BOUNDS, yaw_angle_degree=150)
    info = RobotExecution(action=action)
    return info

## Example 3
# Instruction: Rearrange to this {scene} then restore.
def main_3() -> dict:
    """Execute the given instructions of rearranging the objects to match the objects in the given scene"""
    image_obs = GetObsImage(obs)
    image_goal = templates.get("scene")
    masks_obs = SAM(image=image_obs)
    objs_obs, masks_obs = ImageCrop(image=image_obs, masks=masks_obs)
    masks_goal = SAM(image=image_goal)
    objs_goal, masks_goal = ImageCrop(image=image_goal, masks=masks_goal)
    row, col = get_objs_match(objs_list1=objs_goal, objs_list2=objs_obs)
    action_1 = DistractorActions(mask_obs=masks_obs, obj_list=col)
    action_2 = RearrangeActions(pick_masks=masks_obs, place_masks=masks_goal, pick_ind=col, place_ind=row, bounds=BOUNDS)
    action_3 = RearrangeActions(pick_masks=masks_goal, place_masks=masks_obs, pick_ind=row, place_ind=col, bounds=BOUNDS)
    actions = []
    actions.extend(action_1).extend(action_2).extend(action_3)
    info = RobotExecution(action=actions)
    return info

## Example 4
# Instruction: Put the yellow and blue stripe object in {scene} into the orange object.
def main_4() -> dict:
    """Execute the given instructions of placing the yellow and blue stripe object in scene into the orange object"""
    image = GetObsImage(obs)
    masks_obs = SAM(image=image)
    objs_obs, masks_obs = ImageCrop(image=image, masks=masks_obs)
    objs_goal, masks_goal = ImageCrop(image=templates['scene'], masks=SAM(image=templates['scene']))
    goal = CLIPRetrieval(objs=objs_goal, query='the yellow and blue stripe object')
    target = CLIPRetrieval(objs=objs_obs, query=objs_goal[goal])
    loc_0 = Pixel2Loc(obj=target, masks=masks_obs)
    obj_1 = CLIPRetrieval(objs=objs_obs, query='the orange object', pre_obj1=target)
    loc_1 = Pixel2Loc(obj=obj_1, masks=masks_obs)
    action = PickPlace(pick=loc_0, place=loc_1, bounds=BOUNDS)
    info = RobotExecution(action=action)
    return info

## Example 5
# Instruction: Put the {dragged_obj} into the {base_obj_1} then {base_obj_2}. Finally restore it into its original container.
def mian_5() -> dict:
    image = GetObsImage(obs)
    masks = SAM(image)
    objs, masks = ImageCrop(obs_image, masks)
    base_obj_1 = CLIPRetrieval(objs, templates['base_obj_1'])
    base_obj_2 = CLIPRetrieval(objs, templates['base_obj_2'], pre_obj1=base_obj_1)
    dragged_obj = CLIPRetrieval(objs, templates['dragged_obj'], pre_obj1=base_obj_1, pre_obj2=base_obj_2)
    loc_base_obj_1 = Pixel2Loc(base_obj_1, masks)
    loc_base_obj_2 = Pixel2Loc(base_obj_2, masks)
    loc_dragged_obj = Pixel2Loc(dragged_obj, masks)
    action_1 = PickPlace(pick=loc_dragged_obj, place=loc_base_obj_1, bounds=BOUNDS)
    action_2 = PickPlace(pick=loc_base_obj_1, place=loc_base_obj_2, bounds=BOUNDS)
    action_3 = PickPlace(pick=loc_base_obj_2, place=loc_dragged_obj, bounds=BOUNDS)
    actions = [action_1, action_2, action_3]

    info = RobotExecution(action=actions)
    return info

Begin to execute the task:
------
Please solve the following instruction step-by-step. You should ONLY implement the main() function and output in the Python-code style. Except the code block, output fewer lines.

Instruction: INSERT TASK HERE